{"metadata":{"kernelspec":{"display_name":"Spark 2.0.0 - Scala 2.11","language":"scala","name":"spark2-scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.11.8"}},"nbformat":4,"nbformat_minor":2,"cells":[{"cell_type":"code","execution_count":0,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"/*\n\n    Using Deeplearning4j for anomaly detection\n    Create a deep learning neural network on Apache Spark with Deeplearning4j\n    Romeo Kienzler | Published July 19, 2017\n \n    https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-3/\n\n\n */"},{"cell_type":"code","execution_count":1,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"var v: INDArray = Nd4j.create(Array(Array(1d, 2d, 3d), Array(4d, 5d, 6d)))\nvar w: INDArray = Nd4j.create(Array(Array(1d, 2d), Array(3d, 4d), Array(5d, 6d)))\n"},{"cell_type":"code","execution_count":2,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"\n\n/*\n * List of input values: 4 training samples with data for 2 input‑neurons each.\n */\nvar input: INDArray = Nd4j.zeros(4, 2)\n/*\n * Corresponding list with expected output values, 4 training samples with\n * data for 2 output‑neurons each.\n */\nvar labels: INDArray = Nd4j.zeros(4, 2);\n/*\n * Create first data set when first input=0 and second input=0.\n */\ninput.putScalar(Array(0, 0), 0);\ninput.putScalar(Array(0, 1), 0);\n/*\n * Then the first output fires for false, and the second is 0 (see class comment).\n */\nlabels.putScalar(Array(0, 0), 1);\nlabels.putScalar(Array(0, 1), 0);\n/*\n * When first input=1 and second input=0.\n */\ninput.putScalar(Array(1, 0), 1);\ninput.putScalar(Array(1, 1), 0);\n/*\n * Then XOR is true, therefore the second output neuron fires.\n */\nlabels.putScalar(Array(1, 0), 0);\nlabels.putScalar(Array(1, 1), 1);\n/*\n * Same as above.\n */\ninput.putScalar(Array(2, 0), 0);\ninput.putScalar(Array(2, 1), 1);\nlabels.putScalar(Array(2, 0), 0);\nlabels.putScalar(Array(2, 1), 1);\n/*\n * When both inputs fire, XOR is false again. The first output should fire.\n */\ninput.putScalar(Array(3, 0), 1);\ninput.putScalar(Array(3, 1), 1);\nlabels.putScalar(Array(3, 0), 1);\nlabels.putScalar(Array(3, 1), 0);"},{"cell_type":"code","execution_count":3,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"/*\n * Now, let’s use the data we’ve created above for neural network training.\n */\nimport org.nd4j.linalg.dataset.DataSet\n\nvar ds: DataSet = new DataSet(input, labels)\nprint(ds)"},{"cell_type":"code","execution_count":4,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"import org.deeplearning4j.nn.conf.NeuralNetConfiguration\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration.Builder\nimport org.deeplearning4j.nn.api.OptimizationAlgorithm\nimport org.deeplearning4j.nn.conf.NeuralNetConfiguration.ListBuilder\nimport org.deeplearning4j.nn.conf.layers.DenseLayer\nimport org.nd4j.linalg.activations.Activation\nimport org.deeplearning4j.nn.weights.WeightInit\nimport org.deeplearning4j.nn.conf.distribution.UniformDistribution\n\n\n/*\n * Set up network configuration.\n */\nvar builder: NeuralNetConfiguration.Builder = new NeuralNetConfiguration.Builder();\n\n/*\n * How often should the training set be run? We need something above\n * 1000, or a higher learning‑rate; found this value just by trial and error.\n */\nbuilder.iterations(10000);\n\n/*\n * Learning rate.\n */\nbuilder.learningRate(0.1);\n\n/*\n * Fixed seed for the random generator. Any run of this program\n * brings the same results. Might not work if you do something like ds.shuffle()\n */\nbuilder.seed(123);\n\n/*\n * Not applicable as this network is too small, but for bigger networks it\n * can help that the network is less prone to overfitting to the training data.\n */\nbuilder.useDropConnect(false);\n\n/*\n * A standard algorithm for moving on the error‑plane. This one works\n * best for me. LINE_GRADIENT_DESCENT or CONJUGATE_GRADIENT can do the\n * job, too. It's an empirical value which one matches best to\n * your problem.\n */\nbuilder.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT);\n\n/*\n * Initialize the bias with 0; empirical value, too.\n */\nbuilder.biasInit(0);\n\n/*\n * From \"http://deeplearning4j.org/architecture\": The networks can\n * process the input more quickly and more accurately by ingesting\n * minibatches of 5‑10 elements at a time in parallel.\n * This example runs better without, because the data set is smaller than\n * the minibatch size.\n */\nbuilder.miniBatch(false);\n\n/*\n * Create a multilayer network with two layers (including the output layer, excluding the input layer)\n */\nvar listBuilder: ListBuilder = builder.list();\nvar hiddenLayerBuilder: DenseLayer.Builder = new DenseLayer.Builder();\n\n/*\n * Two input connections simultaneously defines the number of input\n * neurons, because it's the first non‑input‑layer.\n */\nhiddenLayerBuilder.nIn(2);\n\n/*\n * Number of outgoing connections, nOut simultaneously defines the\n * number of neurons in this layer.\n */\nhiddenLayerBuilder.nOut(4);\n\n/*\n * Put the output through the sigmoid function, to cap the output\n * value between 0 and 1.\n */\nhiddenLayerBuilder.activation(Activation.SIGMOID);\n\n/*\n * Random initialize weights with values between 0 and 1.\n */\nhiddenLayerBuilder.weightInit(WeightInit.DISTRIBUTION);\nhiddenLayerBuilder.dist(new UniformDistribution(0, 1));\n\n"},{"cell_type":"code","execution_count":5,"metadata":{"autoscroll":"auto"},"outputs":[],"source":"import org.deeplearning4j.nn.conf.layers.OutputLayer.Builder\n//import org.deeplearning4j.nn.conf.layers.OutputLayer.Builder\nimport org.deeplearning4j.nn.layers.BaseLayer\nimport org.nd4j.linalg.lossfunctions.LossFunctions\n\n/*\n * Build and set as layer 0.\n */\nlistBuilder.layer(0, hiddenLayerBuilder.build());\n\n/*\n MCXENT or NEGATIVELOGLIKELIHOOD (both are mathematically equivalent) work for this example. This\n function calculates the error‑value (or 'cost' or 'loss function value'), and quantifies\nthe goodness\n or badness of a prediction, in a differentiable way.\n For classification (with mutually exclusive classes, like here), use multiclass cross entropy, in conjunction\n with softmax activation function.\n*/\nvar outputLayerBuilder: Builder = new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD);\n\n/*\n Must be the same amount of neurons in the layer before.\n*/\noutputLayerBuilder.nIn(4);\n\n/*\n Two neurons in this layer.\n*/\noutputLayerBuilder.nOut(2);\noutputLayerBuilder.activation(Activation.SOFTMAX);\noutputLayerBuilder.weightInit(WeightInit.DISTRIBUTION);\noutputLayerBuilder.dist(new UniformDistribution(0, 1));\nlistBuilder.layer(1, outputLayerBuilder.build());"}]}